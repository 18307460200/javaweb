---
title: 分布式技术实现
date: 2018/06/01
categories:
- os
tags:
- os
- 分布式
---

# 分布式技术实现

<!-- TOC depthFrom:2 depthTo:3 -->

- [分布式理论](#分布式理论)
    - [CAP 理论](#cap-理论)
    - [BASE 理论](#base-理论)
- [分布式事务](#分布式事务)
    - [两阶段提交（2PC）](#两阶段提交2pc)
    - [补偿事务（TCC）](#补偿事务tcc)
    - [本地消息表（异步确保）](#本地消息表异步确保)
    - [MQ 事务消息](#mq-事务消息)
    - [Sagas 事务模型](#sagas-事务模型)
- [分布式锁](#分布式锁)
    - [基于数据库实现分布式锁](#基于数据库实现分布式锁)
    - [基于缓存（redis，memcached 等）实现](#基于缓存redismemcached-等实现)
    - [基于 ZooKeeper 实现分布式锁](#基于-zookeeper-实现分布式锁)
- [分布式 Session](#分布式-session)
    - [实现](#实现)
- [分布式存储](#分布式存储)
    - [数据分布](#数据分布)
    - [数据复制](#数据复制)
- [分布式计算](#分布式计算)
- [负载均衡](#负载均衡)
    - [算法](#算法)
    - [实现](#实现-1)
- [资料](#资料)

<!-- /TOC -->

## 分布式理论

### CAP 理论

WEB 服务无法同时满足一下 3 个属性：

- 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效)。
- 可用性(Availability) ： 每个操作都必须以可预期的响应结束。
- 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成。

具体地讲在分布式系统中，在任何数据库设计中，一个 Web 应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。

**这个定理在迄今为止的分布式系统中都是适用的！**

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式理论-CAP.jpg" width="400"/>
</div>

### BASE 理论

在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高，那么如何实现高可用性呢？ 前人已经给我们提出来了另外一个理论，就是 BASE 理论，它是用来对 CAP 定理进行进一步扩充的。BASE 理论指的是：

- Basically Available（基本可用）
- Soft state（软状态）
- Eventually consistent（最终一致性）

BASE 理论是对 CAP 中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：**我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。**

## 分布式事务

在分布式系统中，要实现分布式事务，无外乎那几种解决方案。

### 两阶段提交（2PC）

两阶段提交就是使用 XA 协议的原理。

#### XA 协议（XA Transactions）

XA 协议是多数数据库的 2PC 协议的实现，包含了事务管理器和本地资源管理器。

XA 是一个两阶段提交协议，该协议分为以下两个阶段：

（1）准备阶段：事务协调器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交.

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式事务两阶段提交-01.jpg" />
</div>

（2）提交阶段：事务协调器要求每个数据库提交数据。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式事务两阶段提交-02.jpg" />
</div>

需要注意的是，在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。

#### 2PC 分析

2PC 可以保证强一致性，但是因为在准备阶段协调者需要等待所有参与者的结果才能进入提交阶段，因此可用性差。

参与者发生故障。解决方案：可以给事务设置一个超时时间，如果某个参与者一直不响应，那么认为事务执行失败。

协调者发生故障。解决方案：将操作日志同步到备用协调者，让备用协调者接替后续工作。

### 补偿事务（TCC）

TCC 核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段：

1.  Try 阶段主要是对业务系统做检测及资源预留
2.  Confirm 阶段主要是对业务系统做确认提交，Try 阶段执行成功并开始执行 Confirm 阶段时，默认 Confirm 阶段是不会出错的。即：只要 Try 成功，Confirm 一定成功。
3.  Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。

举个例子，假入 Bob 要向 Smith 转账，思路大概是：

我们有一个本地方法，里面依次调用

1.  首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。
2.  在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。
3.  如果第 2 步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。

#### TCC 分析

优点： 跟 2PC 比起来，实现以及流程相对简单了一些，但数据的一致性比 2PC 也要差一些

缺点： 缺点还是比较明显的，在 2,3 步中都有可能失败。TCC 属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用 TCC 不太好定义及处理。

### 本地消息表（异步确保）

本地消息表这种实现方式应该是业界使用最多的，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于 ebay。我们可以从下面的流程图中看出其中的一些细节：

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式事务本地消息.jpg" />
</div>

基本思路就是：

消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过 MQ 发送到消息的消费方。如果消息发送失败，会进行重试发送。

消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。

生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。

这种方案遵循 BASE 理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像 2PC 那样复杂的实现(当调用链很长的时候，2PC 的可用性是非常低的)，也不会像 TCC 那样可能出现确认或者回滚不了的情况。

#### 本地消息表分析

- 优点：一种非常经典的实现，避免了分布式事务，实现了最终一致性。
- 缺点：消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。

### MQ 事务消息

有一些第三方的 MQ 是支持事务消息的，比如 RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，但是市面上一些主流的 MQ 都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。

以阿里的 RocketMQ 中间件为例，其思路大致为：

- 第一阶段 Prepared 消息，会拿到消息的地址。
- 第二阶段执行本地事务，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。

也就是说在业务方法内要想消息队列提交两次请求，一次发送消息和一次确认消息。如果确认消息发送失败了 RocketMQ 会定期扫描消息集群中的事务消息，这时候发现了 Prepared 消息，它会向消息发送者确认，所以生产方需要实现一个 check 接口，RocketMQ 会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式事务MQ事务消息.png" />
</div>

#### MQ 事务消息分析

有关 RocketMQ 的更多消息，大家可以查看这篇博客：http://www.jianshu.com/p/453c6e7ff81c

- 优点：实现了最终一致性，不需要依赖本地数据库事务。
- 缺点：实现难度大，主流 MQ 不支持。

### Sagas 事务模型

Saga 事务模型又叫做长时间运行的事务（Long-running-transaction）, 它是由普林斯顿大学的 H.Garcia-Molina 等人提出，它描述的是另外一种在没有两阶段提交的的情况下解决分布式系统中复杂的业务事务问题。你可以在[这里](https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf)看到 Sagas 相关论文。

我们这里说的是一种基于 Sagas 机制的工作流事务模型，这个模型的相关理论目前来说还是比较新的，以至于百度上几乎没有什么相关资料。

该模型其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，然后由 Sagas 工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，那么 Sagas 工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。

比如我们一次关于购买旅游套餐业务操作涉及到三个操作，他们分别是预定车辆，预定宾馆，预定机票，他们分别属于三个不同的远程接口。可能从我们程序的角度来说他们不属于一个事务，但是从业务角度来说是属于同一个事务的。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式事务Sagas事务模型.png" />
</div>

他们的执行顺序如上图所示，所以当发生失败时，会依次进行取消的补偿操作。

因为长事务被拆分了很多个业务流，所以 Sagas 事务模型最重要的一个部件就是工作流或者你也可以叫流程管理器（Process Manager），工作流引擎和 Process Manager 虽然不是同一个东西，但是在这里，他们的职责是相同的。

## 分布式锁

Java 原生 API 虽然有并发锁，但并没有提供分布式锁的能力，所以针对分布式场景中的锁需要解决的方案。

分布式锁的解决方案大致有以下几种：

- 基于数据库实现
- 基于缓存（redis，memcached 等）实现
- 基于 Zookeeper 实现

### 基于数据库实现分布式锁

#### 实现

##### 1. 创建表

```sql
CREATE TABLE `methodLock` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
  `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名',
  `desc` varchar(1024) NOT NULL DEFAULT '备注信息',
  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的方法';
```

##### 2. 获取锁

想要锁住某个方法时，执行以下 SQL：

```sql
insert into methodLock(method_name,desc) values (‘method_name’,‘desc’)
```

Java 代码：

```java
public boolean lock(){
    connection.setAutoCommit(false)
    while(true){
        try{
            result = select * from methodLock where method_name=xxx for update;
            if(result==null){
                return true;
            }
        }catch(Exception e){

        }
        sleep(1000);
    }
    return false;
}
```

因为我们对 method_name 做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。

##### 3. 释放锁

当方法执行完毕之后，想要释放锁的话，需要执行以下 Sql:

```sql
delete from methodLock where method_name ='method_name'
```

Java 代码：

```java
public void unlock(){
    connection.commit();
}
```

#### 问题

1.  这把锁强依赖数据库的可用性。如果数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。
2.  这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。
3.  这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。
4.  这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。

#### 解决办法

1.  单点问题可以用多数据库实例，同时塞 N 个表，N/2+1 个成功就任务锁定成功
2.  写一个定时任务，隔一段时间清除一次过期的数据。
3.  写一个 while 循环，不断的重试插入，直到成功。
4.  在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。

#### 小结

- 优点: 直接借助数据库，容易理解。
- 缺点: 会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。操作数据库需要一定的开销，性能问题需要考虑。

### 基于缓存（redis，memcached 等）实现

相比于用数据库来实现分布式锁，基于缓存实现的分布式锁的性能会更好一些。目前有很多成熟的分布式产品，包括 Redis、memcache、Tair 等。

#### 实现

单点实现步骤：

1.  获取锁的使用，使用 setnx 加锁，将值设为当前的时间戳，再使用 expire 设置一个过期值。
2.  获取到锁则执行同步代码块，没获取则根据业务场景可以选择自旋、休眠、或做一个等待队列等拥有锁进程来唤醒（类似 Synchronize 的同步队列）,在等待时使用 ttl 去检查是否有过期值，如果没有则使用 expire 设置一个。
3.  执行完毕后，先根据 value 的值来判断是不是自己的锁，如果是的话则删除，不是则表明自己的锁已经过期，不需要删除。（此时出现由于过期而导致的多进程同时拥有锁的问题）

##### 获取锁

setnx 来创建一个 key，如果 key 不存在则创建成功返回 1，如果 key 已经存在则返回 0。依照上述来判定是否获取到了锁

```java
public void lock(){
    while(true){
        ret = set lock_key identify_value nx ex lock_timeout
        if(ret){
            //获取到了锁
            return;
        }
        sleep(100);
    }
}
```

##### 释放锁

完毕后删除 lock_key，来实现释放锁

```java
public void release(){
    value = get lock_key
    if(identify_value == value){
        del lock_key
    }
}
```

#### 问题

- 单点问题。如果单机 redis 挂掉了，那么程序会跟着出错。
- 如果转移使用 slave 节点，复制不是同步复制，会出现多个程序获取锁的情况

#### 小结

可以考虑使用 [redisson 的解决方案](https://github.com/redisson/redisson/wiki/8.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%92%8C%E5%90%8C%E6%AD%A5%E5%99%A8)。

### 基于 ZooKeeper 实现分布式锁

#### 实现

这也是 ZooKeeper 客户端 curator 的分布式锁实现。

##### 获取锁

```java
public void lock(){
    path = 在父节点下创建临时顺序节点
    while(true){
        children = 获取父节点的所有节点
        if(path是children中的最小的){
            代表获取了节点
            return;
        }else{
            添加监控前一个节点是否存在的watcher
            wait();
        }
    }
}

watcher中的内容{
    notifyAll();
}
```

##### 释放锁

```java
public void release(){
    删除上述创建的节点
}
```

#### 小结

ZooKeeper 版本的分布式锁问题相对比较来说少。

- 锁的占用时间限制：redis 就有占用时间限制，而 ZooKeeper 则没有，最主要的原因是 redis 目前没有办法知道已经获取锁的客户端的状态，是已经挂了呢还是正在执行耗时较长的业务逻辑。而 ZooKeeper 通过临时节点就能清晰知道，如果临时节点存在说明还在执行业务逻辑，如果临时节点不存在说明已经执行完毕释放锁或者是挂了。由此看来 redis 如果能像 ZooKeeper 一样添加一些与客户端绑定的临时键，也是一大好事。
- 是否单点故障：redis 本身有很多中玩法，如客户端一致性 hash，服务器端 sentinel 方案或者 cluster 方案，很难做到一种分布式锁方式能应对所有这些方案。而 ZooKeeper 只有一种玩法，多台机器的节点数据是一致的，没有 redis 的那么多的麻烦因素要考虑。

总体上来说 ZooKeeper 实现分布式锁更加的简单，可靠性更高。

## 分布式 Session

在分布式场景下，一个用户的 Session 如果只存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器上，该服务器没有用户的 Session，就可能导致用户需要重新进行登录等操作。

### 实现

分布式 Session 的几种实现策略：

1.  粘性 session
2.  应用服务器间的 session 复制共享
3.  基于 cache DB 缓存的 session 共享

#### 1. 粘性 session

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式Session之粘性Session.png" width="640"/>
</div>

- 原理：粘性 Session 是指将用户锁定到某一个服务器上，用户第一次请求时，负载均衡器将用户的请求转发到了 A 服务器上，那么用户以后的每次请求都会转发到 A 服务器上。
- 优点：简单，不需要对 session 做任何处理。
- 缺点：缺乏容错性，如果当前访问的服务器发生故障，用户被转移到第二个服务器上时，他的 session 信息都将失效。
- 实现方式：以 Nginx 为例，在 upstream 模块配置 ip_hash 属性即可实现粘性 Session。

```nginx
upstream load_balance_server {
    server 127.0.0.1:8001;
    server 127.0.0.1:9001;
    ip_hash;
}
server {
    listen       100;
    #定义使用www.xx.com访问
    #server_name www.helloworld.com;
    server_name  localhost;

    location / {
        root   html;
        index  index.html index.htm;
        #请求转向load_balance_server 定义的服务器列表
        proxy_pass http://load_balance_server ;

        #以下是一些反向代理的配置(可选择性配置)
        proxy_redirect off;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

#### 2. 应用服务器间的 session 复制共享

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式Session之复制共享Session.png" width="640"/>
</div>

原理：任何一个服务器上的 session 发生改变（增删改），该节点会把这个 session 的所有内容序列化，然后广播给所有其它节点，不管其他服务器需不需要 session，以此来保证 Session 同步。

优点：可容错，各个服务器间 session 能够实时响应。

缺点：会对网络负荷造成一定压力，如果 session 量大的话可能会造成网络堵塞，拖慢服务器性能。

实现方式：本文以 Tomcat8 集群为例。 step1：复制 app-session 的 war 包解压到 tomcat8 webapps ROOT 目录下,tomcat 复制两份,然后修改 tomcat 的 server.xml 文件。
tomcat1 server.xml 改动点:

```xml
<!-- 同一台机器上需要保证shutdown  ajp等端口不一样 -->
  <Server port="8706" shutdown="SHUTDOWN">
  <Connector port="9002" protocol="HTTP/1.1"
                 connectionTimeout="20000"
                 redirectPort="9746" />

 <Connector port="8009" protocol="AJP/1.3" redirectPort="8443" />

  <!-- Engine的jvmRoute 参数名称与其它服务区分 -->
  <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat1">

 <Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster" channelSendOptions="8">
      <Manager className="org.apache.catalina.ha.session.DeltaManager"
      expireSessionsOnShutdown="false"
      notifyListenersOnReplication="true"/>

      <!--228.0.0.4 保留ip,用于广播-->
      <Channel className="org.apache.catalina.tribes.group.GroupChannel">
          <Membership className="org.apache.catalina.tribes.membership.McastService"
              address="228.0.0.4"
              port="45564"
              frequency="500"
              dropTime="3000"/>

          <!-- port 如果是在同一台机器上的两个tomcat做负载，则此端口则不能重复-->
          <Receiver className="org.apache.catalina.tribes.transport.nio.NioReceiver"
              address="127.0.0.1"
              port="4008"
              autoBind="100"
              selectorTimeout="5000"
              maxThreads="6"/>

          <Sender className="org.apache.catalina.tribes.transport.ReplicationTransmitter">
              <Transport className="org.apache.catalina.tribes.transport.nio.PooledParallelSender"/>
          </Sender>
          <Interceptor className="org.apache.catalina.tribes.group.interceptors.TcpFailureDetector"/>
          <Interceptor className="org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor"/>
      </Channel>

      <Valve className="org.apache.catalina.ha.tcp.ReplicationValve" filter=""/>
      <Valve className="org.apache.catalina.ha.session.JvmRouteBinderValve"/>

      <Deployer className="org.apache.catalina.ha.deploy.FarmWarDeployer"
          tempDir="/tmp/war-temp/"
          deployDir="/tmp/war-deploy/"
          watchDir="/tmp/war-listen/"
          watchEnabled="false"/>
      <ClusterListener className="org.apache.catalina.ha.session.ClusterSessionListener"/>
  </Cluster>
```

tomcat2 server.xml

```xml
<Server port="8006" shutdown="SHUTDOWN">
<Connector port="8002" protocol="HTTP/1.1"
               connectionTimeout="20000"
               redirectPort="9446" />
<Connector port="8019" protocol="AJP/1.3" redirectPort="8444" />
 <!-- Engine的jvmRoute 参数名称与其它服务区分 -->
<Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat2">

<Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster" channelSendOptions="8">
    <Manager className="org.apache.catalina.ha.session.DeltaManager"
    expireSessionsOnShutdown="false"
    notifyListenersOnReplication="true"/>

    <!--228.0.0.4 保留ip,用于广播-->
    <Channel className="org.apache.catalina.tribes.group.GroupChannel">
        <Membership className="org.apache.catalina.tribes.membership.McastService"
            address="228.0.0.4"
            port="45564"
            frequency="500"
            dropTime="3000"/>

        <!-- port 如果是在同一台机器上的两个tomcat做负载，则此端口则不能重复-->
        <Receiver className="org.apache.catalina.tribes.transport.nio.NioReceiver"
            address="127.0.0.1"
            port="4002"
            autoBind="100"
            selectorTimeout="5000"
            maxThreads="6"/>

        <Sender className="org.apache.catalina.tribes.transport.ReplicationTransmitter">
            <Transport className="org.apache.catalina.tribes.transport.nio.PooledParallelSender"/>
        </Sender>
        <Interceptor className="org.apache.catalina.tribes.group.interceptors.TcpFailureDetector"/>
        <Interceptor className="org.apache.catalina.tribes.group.interceptors.MessageDispatch15Interceptor"/>
    </Channel>

    <Valve className="org.apache.catalina.ha.tcp.ReplicationValve" filter=""/>
    <Valve className="org.apache.catalina.ha.session.JvmRouteBinderValve"/>

    <Deployer className="org.apache.catalina.ha.deploy.FarmWarDeployer"
        tempDir="/tmp/war-temp/"
        deployDir="/tmp/war-deploy/"
        watchDir="/tmp/war-listen/"
        watchEnabled="false"/>
    <ClusterListener className="org.apache.catalina.ha.session.ClusterSessionListener"/>
</Cluster>
```

#### 3. 基于 cache DB 缓存的 session 共享

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/分布式Session之缓存Session.png" width="640"/>
</div>

- 原理：session 持久化到缓存或者 db 中。
- 优点：gemfire,memcache 或则 redis 本身就是一个分布式缓存，便于扩展。网络开销较小，IO 开销也非常小，性能也更好。服务器出现问题，session 不会丢失。
- 缺点：假如突然涌来大量用户产生了很多数据把存储 session 的机器内存占满了 redis 会变的比较慢
- 实现方式：spring-session-data-redis

## 分布式存储

通常有两种解决方案：

1.  数据分布：就是把数据分块存在不同的服务器上（分库分表）。
2.  数据复制：让所有的服务器都有相同的数据，提供相当的服务。

### 数据分布

#### 一致性哈希算法

算法思想如下：给系统中每个节点分配一个随机 token，这些 token 构成一个哈希环。执行数据存放操作时，先计算 Key(主键)的哈希值，然后存放到顺时针方向第一个大于或者等于该哈希值的 token 所在的节点。一致性哈希的优点在于节点加入 / 删除时只会影响到在哈希环中相邻的节点，而对其他节点没影响。增加节点后能很大程度上避免了数据迁移。为了考虑负载均衡，一般还会引入虚拟节点的技术，即一个物理节点会对应着多个虚拟节点（如 Dynamo）。

### 数据复制

## 分布式计算

## 负载均衡

### 算法

#### 1. 轮询（Round Robin）

轮询算法把每个请求轮流发送到每个服务器上。下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。最后，(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/负载均衡算法之轮询-01.jpg" width="640"/>
</div>

该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载（下图的 Server 2）。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/负载均衡算法之轮询-02.jpg" width="640"/>
</div>

#### 2. 加权轮询（Weighted Round Robbin）

加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值。例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/负载均衡算法之加权轮询.jpg" width="640"/>
</div>

#### 3. 最少连接（least Connections）

由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开。该系统继续运行时，服务器 2 会承担过大的负载。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/负载均衡算法之最少连接-01.jpg" width="640"/>
</div>

最少连接算法就是将请求发送给当前最少连接数的服务器上。例如下图中，服务器 1 当前连接数最小，那么新到来的请求 6 就会被发送到服务器 1 上。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/负载均衡算法之最少连接-02.jpg" width="640"/>
</div>

#### 4. 加权最少连接（Weighted Least Connection）

在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/负载均衡算法之加权最少连接.jpg" width="640"/>
</div>

#### 5. 随机算法（Random）

把请求随机发送到服务器上。和轮询算法类似，该算法比较适合服务器性能差不多的场景。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/负载均衡算法之随机.jpg" width="640"/>
</div>

#### 6. 源地址哈希法 (IP Hash)

源地址哈希通过对客户端 IP 哈希计算得到的一个数值，用该数值对服务器数量进行取模运算，取模结果便是目标服务器的序号。

- 优点：保证同一 IP 的客户端都会被 hash 到同一台服务器上。
- 缺点：不利于集群扩展，后台服务器数量变更都会影响 hash 结果。可以采用一致性 Hash 改进。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/负载均衡算法之IpHash.jpg" width="640"/>
</div>

### 实现

#### 1. HTTP 重定向

HTTP 重定向负载均衡服务器收到 HTTP 请求之后会返回服务器的地址，并将该地址写入 HTTP 重定向响应中返回给浏览器，浏览器收到后需要再次发送请求。

缺点：

- 用户访问的延迟会增加；
- 如果负载均衡器宕机，就无法访问该站点。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/Http重定向.png" width="640"/>
</div>

#### 2. DNS 重定向

使用 DNS 作为负载均衡器，根据负载情况返回不同服务器的 IP 地址。大型网站基本使用了这种方式做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。

缺点：

- DNS 查找表可能会被客户端缓存起来，那么之后的所有请求都会被重定向到同一个服务器。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/Dns重定向.png" width="640"/>
</div>

#### 3. 修改 MAC 地址

使用 LVS（Linux Virtual Server）这种链路层负载均衡器，根据负载情况修改请求的 MAC 地址。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/修改Mac地址.png" width="640"/>
</div>

#### 4. 修改 IP 地址

在网络层修改请求的目的 IP 地址。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/修改IP地址.png" width="640"/>
</div>

#### 5. 代理自动配置

正向代理与反向代理的区别：

- 正向代理：发生在客户端，是由用户主动发起的。比如翻墙，客户端通过主动访问代理服务器，让代理服务器获得需要的外网数据，然后转发回客户端。
- 反向代理：发生在服务器端，用户不知道代理的存在。

PAC 服务器是用来判断一个请求是否要经过代理。

<div align="center">
<img src="https://raw.githubusercontent.com/dunwu/JavaWeb/master/images/distributed/architecture/代理自动配置.jpg" width="640"/>
</div>

## 资料

- https://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html
- https://github.com/CyC2018/Interview-Notebook/blob/master/notes/%E5%88%86%E5%B8%83%E5%BC%8F%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90.md
- https://www.jianshu.com/p/453c6e7ff81c
- https://juejin.im/post/5a20cd8bf265da43163cdd9a
- https://github.com/redisson/redisson/wiki/8.-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%92%8C%E5%90%8C%E6%AD%A5%E5%99%A8
- https://github.com/L316476844/distributed-session
